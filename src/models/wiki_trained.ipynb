{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Politics', 'Health', 'Finance', 'Travel', 'Food', 'Education','Environment', 'Fashion', 'Science', 'Sports', 'Technology', 'Entertainment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = {}\n",
    "articles_dict[\"Politics\"] = [\"Politics\",\"Democracy\",\"Communism\",\"Fascism\",\"Totalitarianism\",\"Political corruption\",\"Liberalism\",\"Conservatism\",\"Socialism\",\"Anarchism\",\"Civic nationalism\",\"Ethnic nationalism\",\"Feminism\",\"Environmental politics\",\"Political campaign\",\"Voter turnout\",\"Election fraud\",\"Human rights\",\" Nations\",\"European Union\",\"Brexit\",\"NATO\",\" War\",\"World War II\",\" War\",\"American Civil War\",\"French Revolution\",\" Revolution (1917)\",\"Chinese Revolution\",\"Cuban Revolution\",\"Apartheid in South Africa\",\"Civil Rights Movement in the United States\",\"Indian Independence Movement\",\"Arab Spring\",\"Syrian Civil War\",\"Israeli–Palestinian conflict\",\"North Korea–South Korea relations\",\"Iran nuclear deal\",\"War on Terror\",\"Climate change policy\",\"Digital privacy\",\"Whistleblowing\",\"Nonviolent resistance\",\"Terrorism\",\"Authoritarianism\",\"Electoral system\",\"Political party\",\"Public policy\",\"Separation of powers\",\"Judicial independence\",\"Constitution\",\"Bill of Rights\",\"Supreme Court of the United States\",\"Vladimir Putin\",\"Barack Obama\",\"Margaret Thatcher\",\"Mao Zedong\",\"Nelson Mandela\",\"Winston Churchill\",\"Che Guevara\",\"Fidel Castro\",\"Theodore Roosevelt\",\"Joseph Stalin\",\"Gandhi\",\"Julius Caesar\",\"Benjamin Franklin\",\"Political philosophy\",\"Political science\",\"Public administration\",\"Civic education\",\"International relations\",\"Diplomacy\",\"Globalization\",\"Trade agreements\",\"Sanctions\",\"Political asylum\",\"Refugee crisis\",\"Nation-state\",\"Sovereignty\",\"Colonialism\",\"Imperialism\",\"Populism\",\"Nationalism\",\"Identity politics\",\"Political activism\",\"Lobbying\",\"United States Congress\",\"Parliament of the United Kingdom\",\"Bundestag\",\"Duma\",\"Political economy\",\"Market economy\",\"Command economy\",\"Mixed economy\",\"Economic policy\",\"Income inequality\",\"Wealth distribution\",\"Welfare state\",\"Universal basic income\",\"Political ideologies\",\"Political theory\"]\n",
    "articles_dict[\"Health\"] = [\"Health\", \"Medicine\", \"Public health\", \"Health care\", \"Epidemiology\", \"Global health\", \"Mental health\", \"Nutrition\", \"Disease\", \"Obesity\", \"Diabetes mellitus\", \"Cardiovascular disease\", \"Cancer\", \"HIV/AIDS\", \"Malaria\", \"Tuberculosis\", \"Vaccination\", \"Health psychology\", \"Dental care\", \"Pediatrics\", \"Geriatrics\", \"Nursing\", \"Surgery\", \"Pharmacy\", \"Biomedicine\", \"Telemedicine\", \"Health education\", \"Physical fitness\", \"Smoking cessation\", \"Alcohol abuse\", \"Drug abuse\", \"Mental disorders\", \"Anxiety\", \"Depression\", \"Bipolar disorder\", \"Schizophrenia\", \"Autism\", \"Maternal health\", \"Childbirth\", \"Genetic disorders\", \"Infectious diseases\", \"Chronic conditions\", \"Occupational safety and health\", \"Environmental health\", \"Health disparities\", \"Elderly care\", \"Palliative care\", \"First aid\", \"Emergency medicine\", \"Medical imaging\", \"Clinical trial\", \"Epidemic\", \"Pandemic\", \"Health insurance\", \"Universal health care\", \"Alternative medicine\", \"Holistic health\", \"Sports medicine\", \"Orthopedics\", \"Physical therapy\", \"Speech therapy\", \"Dietary supplements\", \"Veganism\", \"Vegetarianism\", \"Nutritional deficiencies\", \"Obstetrics\", \"Gynecology\", \"Neurology\", \"Oncology\", \"Endocrinology\", \"Cardiology\", \"Dermatology\", \"Immunology\", \"Gastroenterology\", \"Hepatology\", \"Nephrology\", \"Urology\", \"Ophthalmology\", \"Otolaryngology\", \"Pulmonology\", \"Rheumatology\", \"Pathology\", \"Medical research\", \"Health policy\", \"Medical ethics\", \"Bioethics\", \"Health promotion\", \"Hygiene\", \"Sanitation\", \"Immunization\", \"Antibiotic resistance\", \"Medical statistics\", \"Health literacy\", \"Patient safety\", \"Healthcare quality\", \"Medical informatics\", \"Biotechnology\", \"Life extension\", \"Aging\", \"Wellness\", \"Preventive healthcare\"]\n",
    "articles_dict[\"Finance\"] = [\"Finance\", \"Personal finance\", \"Corporate finance\", \"Public finance\", \"Financial market\", \"Stock market\", \"Bond market\", \"Commodity market\", \"Foreign exchange market\", \"Investment management\", \"Hedge fund\", \"Mutual fund\", \"Index fund\", \"Financial services\", \"Banking\", \"Insurance\", \"Venture capital\", \"Private equity\", \"Credit\", \"Debt\", \"Loan\", \"Mortgage loan\", \"Interest rate\", \"Credit score\", \"Financial planning\", \"Retirement planning\", \"Tax\", \"Wealth management\", \"Estate planning\", \"Budget\", \"Audit\", \"Financial accounting\", \"Managerial accounting\", \"Financial statement\", \"Balance sheet\", \"Income statement\", \"Cash flow statement\", \"Derivative (finance)\", \"Futures contract\", \"Option (finance)\", \"Swap (finance)\", \"Financial risk management\", \"Financial regulation\", \"Securities regulation\", \"Financial crisis\", \"Great Depression\", \"Financial technology\", \"Blockchain\", \"Cryptocurrency\", \"Bitcoin\", \"Economic growth\", \"Inflation\", \"Deflation\", \"Monetary policy\", \"Central bank\", \"Federal Reserve System\", \"Bank of England\", \"European Central Bank\", \"Investment banking\", \"Credit card\", \"Payment system\", \"Electronic money\", \"Savings account\", \"Checking account\", \"Dividend\", \"Yield (finance)\", \"Return on investment\", \"Capital gain\", \"Capital market\", \"Money market\", \"Risk management\", \"Financial analyst\", \"Financial adviser\", \"Financial independence\", \"Wealth\", \"Poverty\", \"Financial literacy\", \"Forex trading\", \"Commodities trading\", \"Financial ethics\", \"Corporate governance\", \"Economic indicators\", \"Macroeconomics\", \"Microeconomics\", \"Economic development\", \"Economic policy\", \"Financial modeling\", \"Quantitative analysis\", \"Algorithmic trading\", \"High-frequency trading\", \"Merger and acquisition\", \"Initial public offering\", \"Stock exchange\", \"Bond issuance\", \"Financial innovation\", \"Sustainable finance\", \"Environmental, social, and corporate governance (ESG)\", \"Impact investing\", \"Socially responsible investing\", \"Green finance\", \"Islamic finance\", \"Financial law\", \"Financial economics\", \"Financial markets and institutions\", \"International finance\", \"Global financial system\"]\n",
    "articles_dict[\"Travel\"] = [\"Travel\", \"Tourism\", \"Adventure travel\", \"Backpacking (travel)\", \"Cruise ship\", \"Cultural tourism\", \"Eco-tourism\", \"Dark tourism\", \"Gastronomic tourism\", \"Heritage tourism\", \"Hiking\", \"Hospitality industry\", \"Hotel\", \"Tourist attraction\", \"Leisure travel\", \"Luxury travel\", \"Medical tourism\", \"Nautical tourism\", \"Passport\", \"Road trip\", \"Safari\", \"Solo travel\", \"Space tourism\", \"Sports tourism\", \"Staycation\", \"Sustainable tourism\", \"Tour guide\", \"Tour operator\", \"Tourist visa\", \"Travel agency\", \"Travel document\", \"Travel insurance\", \"Travel journal\", \"Travel literature\", \"Travel photography\", \"Travel technology\", \"Vacation\", \"Visa policy\", \"Voluntourism\", \"World Heritage site\", \"World Tourism rankings\", \"Youth travel\", \"Adventure tourism\", \"Agritourism\", \"Cultural exchange\", \"Ecotourism\", \"Educational tourism\", \"Extreme tourism\", \"Geotourism\", \"Medical tourism\", \"Religious tourism\", \"Slum tourism\", \"Space tourism\", \"Sports tourism\", \"War tourism\", \"Wildlife tourism\", \"Travel and tourism competitiveness index\", \"Travel behavior\", \"Travel behavior research\", \"Travel behavior analysis\", \"Travel behavior modeling\", \"Travel behavior forecasting\", \"Travel behavior simulation\", \"Travel behavior theory\", \"Travel behavior patterns\", \"Travel behavior trends\", \"Travel behavior data\", \"Travel behavior survey\", \"Travel behavior questionnaire\", \"Travel behavior statistics\", \"Travel behavior metrics\", \"Travel behavior measurement\", \"Travel behavior evaluation\", \"Travel behavior optimization\", \"Travel behavior management\", \"Travel behavior planning\", \"Travel behavior policy\", \"Travel behavior impact\", \"Travel behavior change\", \"Travel behavior adaptation\", \"Travel behavior challenges\", \"Travel behavior opportunities\", \"Travel behavior future\", \"Travel behavior research methods\", \"Travel behavior case studies\", \"Travel behavior applications\", \"Travel behavior implications\", \"Travel behavior recommendations\", \"Travel behavior guidelines\", \"Travel behavior best practices\", \"Travel behavior lessons learned\", \"Travel behavior insights\", \"Travel behavior perspectives\", \"Travel behavior reflections\", \"Travel behavior reviews\", \"Travel behavior reports\", \"Travel behavior studies\", \"Travel behavior papers\", \"Travel behavior articles\", \"Travel behavior books\", \"Travel behavior journals\", \"Travel behavior conferences\", \"Travel behavior events\", \"Travel behavior organizations\", \"Travel behavior experts\", \"Travel behavior professionals\", \"Travel behavior consultants\", \"Travel behavior services\", \"Travel behavior solutions\", \"Travel behavior systems\", \"Travel behavior tools\", \"Travel behavior trends\", \"Travel behavior updates\", \"Travel behavior news\", \"Travel behavior blog\", \"Travel behavior website\", \"Travel behavior forum\", \"Travel behavior community\", \"Travel behavior network\", \"Travel behavior portal\", \"Travel behavior platform\", \"Travel behavior resources\", \"Travel behavior links\", \"Travel behavior access\", \"Travel behavior support\", \"Travel behavior help\", \"Travel behavior FAQ\", \"Travel behavior contact\", \"Travel behavior location\", \"Travel behavior map\", \"Travel behavior directions\", \"Travel behavior schedule\", \"Travel behavior calendar\", \"Travel behavior timeline\", \"Travel behavior history\", \"Travel behavior background\", \"Travel behavior overview\", \"Travel behavior introduction\", \"Travel behavior summary\", \"Travel behavior highlights\", \"Travel behavior key points\", \"Travel behavior key takeaways\", \"Travel behavior conclusions\", \"Travel behavior recommendations\", \"Travel behavior references\", \"Travel behavior resources\", \"Travel behavior acknowledgements\", \"Travel behavior glossary\", \"Travel behavior index\", \"Travel behavior appendix\", \"Travel behavior attachments\", \"Travel behavior images\", \"Travel behavior photos\", \"Travel behavior pictures\"]\n",
    "articles_dict[\"Food\"] = [\"Food\", \"Cuisine\", \"Cooking\", \"Baking\", \"Roasting\", \"Grilling\", \"Frying\", \"Simmering\", \"Marinating\", \"Food preservation\", \"Canning\", \"Freezing\", \"Drying\", \"Smoking (cooking)\", \"Fermentation in food processing\", \"Pickling\", \"Culinary arts\", \"Chef\", \"Recipe\", \"Ingredient\", \"Meal\", \"Breakfast\", \"Lunch\", \"Dinner\", \"Snack\", \"Dessert\", \"Fast food\", \"Street food\", \"Gourmet\", \"Diet (nutrition)\", \"Vegetarianism\", \"Veganism\", \"Raw foodism\", \"Organic food\", \"Gluten-free diet\", \"Mediterranean diet\", \"Kosher food\", \"Halal\", \"Seafood\", \"Meat\", \"Poultry\", \"Dairy\", \"Fruit\", \"Vegetable\", \"Grain\", \"Nut (fruit)\", \"Beverage\", \"Alcoholic beverage\", \"Non-alcoholic beverage\", \"Coffee\", \"Tea\", \"Soft drink\", \"Water\", \"Food safety\", \"Food industry\", \"Food processing\", \"Food service\", \"Food festival\", \"Food tourism\", \"Food criticism\", \"Food security\", \"Sustainable agriculture\", \"Agriculture\", \"Food science\", \"Nutrition\", \"Food technology\", \"Food policy\", \"Gastronomy\", \"Molecular gastronomy\", \"Food history\", \"National dishes\", \"World cuisines\", \"Ethnic food\"]\n",
    "articles_dict[\"Education\"] = [\"Education\", \"Academic degree\", \"Primary education\", \"Secondary education\", \"Higher education\", \"Vocational education\", \"Adult education\", \"Special education\", \"Homeschooling\", \"Online education\", \"Distance learning\", \"Curriculum\", \"Pedagogy\", \"Educational technology\", \"Educational psychology\", \"School\", \"University\", \"College\", \"Vocational school\", \"Academic conference\", \"Academic journal\", \"Student\", \"Professor\", \"Lecture\", \"Seminar\", \"Workshop\", \"Textbook\", \"E-learning\", \"MOOC\", \"Educational assessment\", \"Standardized test\", \"Educational policy\", \"Education reform\", \"Scholarship\", \"Student loan\", \"Internship\", \"Apprenticeship\", \"Academic discipline\", \"Liberal arts education\", \"STEM education\", \"Business education\", \"Medical education\", \"Legal education\", \"Engineering education\", \"Art education\", \"Music education\", \"Physical education\", \"Health education\", \"Environmental education\", \"Multicultural education\", \"Inclusive education\", \"Educational inequality\", \"Literacy\", \"Numeracy\", \"Educational standards\", \"Accreditation\", \"Graduation\", \"Academic certificate\", \"Diploma\", \"Educational accreditation\", \"Board of education\", \"Ministry of education\", \"Educational funding\", \"Tuition\", \"School choice\", \"Charter school\", \"Private school\", \"Public school\", \"Education in developing countries\", \"Lifelong learning\", \"Learning styles\", \"Education theory\", \"Educational philosophy\", \"History of education\"]\n",
    "articles_dict[\"Environment\"] = [\"Environment\", \"Environmental science\", \"Ecology\", \"Biodiversity\", \"Conservation\", \"Climate change\", \"Global warming\", \"Renewable energy\", \"Solar energy\", \"Wind energy\", \"Hydropower\", \"Bioenergy\", \"Geothermal energy\", \"Environmental policy\", \"Sustainable development\", \"Recycling\", \"Waste management\", \"Pollution\", \"Air pollution\", \"Water pollution\", \"Soil contamination\", \"Greenhouse gas\", \"Carbon footprint\", \"Environmental impact assessment\", \"Ecosystem\", \"Habitat\", \"Wildlife\", \"Endangered species\", \"Deforestation\", \"Desertification\", \"Ocean conservation\", \"Coral reefs\", \"Environmental activism\", \"Green movement\", \"Earth Day\", \"Environmental law\", \"Environmental ethics\", \"Eco-friendly\", \"Green building\", \"Urban planning\", \"Natural resource management\", \"Water conservation\", \"Sustainable agriculture\", \"Organic farming\", \"Permaculture\", \"Biodegradable\", \"Composting\", \"Environmental education\", \"Environmental health\", \"Environmental technology\", \"Green technology\", \"Climate adaptation\", \"Mitigation of climate change\", \"Sustainability metrics and indices\", \"Environmental economics\", \"Ecotourism\", \"Green consumerism\", \"Green business\", \"Environmental design\", \"Environmental engineering\", \"Nature reserve\", \"National park\", \"Protected area\", \"Wildlife sanctuary\", \"Forest management\", \"Rangeland management\", \"Environmental monitoring\", \"Atmospheric sciences\", \"Environmental chemistry\", \"Earth science\", \"Conservation biology\", \"Marine biology\", \"Landscape ecology\", \"Agroecology\", \"Environmental studies\", \"Environmental psychology\", \"Natural disaster\", \"Erosion control\", \"Flood control\", \"Sustainable energy\", \"Energy conservation\", \"Climate change mitigation\", \"Climate resilience\"]\n",
    "articles_dict[\"Fashion\"] = [\"Fashion\", \"Haute couture\", \"Ready-to-wear\", \"Fashion design\", \"Fashion designer\", \"Fashion week\", \"Fashion show\", \"Fashion model\", \"Fashion photography\", \"Fashion magazine\", \"Fashion trend\", \"Vintage fashion\", \"Street fashion\", \"Luxury goods\", \"Fashion industry\", \"Textile industry\", \"Fashion marketing\", \"Fashion retailing\", \"Fashion styling\", \"Costume design\", \"Clothing\", \"Dress\", \"Suit\", \"T-shirt\", \"Jeans\", \"Skirt\", \"Gown\", \"Footwear\", \"Sneakers\", \"High heels\", \"Fashion accessory\", \"Jewelry\", \"Watch\", \"Handbag\", \"Belt\", \"Hat\", \"Scarf\", \"Eyewear\", \"Sunglasses\", \"Fashion history\", \"Fashion icon\", \"Fashion influencer\", \"Celebrity fashion\", \"Fashion and culture\", \"Fashion and identity\", \"Ethical fashion\", \"Sustainable fashion\", \"Eco fashion\", \"Recycled fashion\", \"Fashion and technology\", \"Wearable technology\", \"Fashion law\", \"Fashion criticism\", \"Fashion theory\", \"Fashion education\", \"Fashion school\", \"Fashion books\", \"Fashion film\", \"Fashion psychology\", \"Men's fashion\", \"Women's fashion\", \"Children's fashion\", \"Teen fashion\", \"Fashion in film\", \"Fashion in music\", \"Fashion aesthetics\", \"Fashion forecasting\", \"Fashion entrepreneurship\", \"Fashion merchandising\", \"Visual merchandising\", \"Fashion buying\", \"Fashion branding\", \"Fashion management\", \"Fashion logistics\", \"Fashion PR\", \"Fashion communication\", \"Fashion photography\", \"Fashion art\", \"Fashion sculpture\"]\n",
    "articles_dict[\"Science\"] = [\"Science\", \"Natural science\", \"Social science\", \"Formal science\", \"Applied science\", \"Physical science\", \"Life science\", \"Earth science\", \"Space science\", \"Environmental science\", \"Computer science\", \"Information science\", \"Health science\", \"Medical science\", \"Biological science\", \"Physics\", \"Chemistry\", \"Biology\", \"Ecology\", \"Zoology\", \"Botany\", \"Geology\", \"Meteorology\", \"Astronomy\", \"Astrophysics\", \"Quantum mechanics\", \"Theoretical physics\", \"Organic chemistry\", \"Inorganic chemistry\", \"Biochemistry\", \"Genetics\", \"Microbiology\", \"Neuroscience\", \"Pharmacology\", \"Epidemiology\", \"Immunology\", \"Marine biology\", \"Paleontology\", \"Materials science\", \"Mathematics\", \"Statistics\", \"Engineering\", \"Technology\", \"Nanotechnology\", \"Biotechnology\", \"Cognitive science\", \"Psychology\", \"Anthropology\", \"Sociology\", \"Economics\", \"Political science\", \"Linguistics\", \"Archaeology\", \"History of science\", \"Philosophy of science\", \"Scientific method\", \"Scientific research\", \"Experimental design\", \"Data analysis\", \"Scientific literature\", \"Scientific community\", \"Science education\", \"Science policy\", \"Science communication\", \"Science outreach\", \"Science and technology studies\", \"Science and society\", \"Scientific ethics\", \"Science funding\", \"Scientific awards\", \"Scientific revolutions\", \"Scientific discoveries\", \"Scientific theories\", \"Scientific laws\", \"Scientific models\", \"Scientific instruments\", \"Laboratory\", \"Scientist\", \"Researcher\", \"Academic journal\", \"Peer review\", \"Scientific publishing\", \"Open science\", \"Citizen science\", \"International science\", \"Global science initiatives\"]\n",
    "articles_dict[\"Sports\"] = [\"Sport\", \"Olympic Games\", \"FIFA World Cup\", \"Basketball\", \"Soccer\", \"Baseball\", \"American football\", \"Ice hockey\", \"Cricket\", \"Rugby football\", \"Tennis\", \"Golf\", \"Track and field athletics\", \"Swimming (sport)\", \"Boxing\", \"Mixed martial arts\", \"Gymnastics\", \"Volleyball\", \"Skiing\", \"Snowboarding\", \"Skateboarding\", \"Surfing\", \"Motorsport\", \"Formula One\", \"Cycling\", \"Mountain biking\", \"Horse racing\", \"Equestrianism\", \"Rowing (sport)\", \"Sailing\", \"Wrestling\", \"Fencing\", \"Archery\", \"Shooting sport\", \"Table tennis\", \"Badminton\", \"Squash (sport)\", \"Bowling\", \"Billiards\", \"Snooker\", \"Chess\", \"Esports\", \"Martial arts\", \"Karate\", \"Judo\", \"Taekwondo\", \"Brazilian jiu-jitsu\", \"Kendo\", \"Dance sport\", \"Bodybuilding\", \"Weightlifting\", \"Powerlifting\", \"Adventure racing\", \"Triathlon\", \"Pentathlon\", \"Decathlon\", \"Heptathlon\", \"Paralympic Games\", \"Youth sports\", \"College sports\", \"University sports\", \"Sports coaching\", \"Sports training\", \"Sports equipment\", \"Sports medicine\", \"Sports psychology\", \"Sports nutrition\", \"Sports management\", \"Sports marketing\", \"Sports law\", \"Sports broadcasting\", \"Sports history\", \"Sports culture\", \"Women in sports\", \"Sports awards\", \"Sports records\", \"Sports leagues\", \"Sports teams\", \"Sports events\", \"National sports\", \"International sports competitions\", \"Sports venues\", \"Stadiums\", \"Arenas\", \"Sports fans\", \"Sports rivalries\", \"Outdoor recreation\", \"Indoor sports\", \"Water sports\", \"Winter sports\", \"Extreme sports\", \"Disabled sports\", \"Mind sports\", \"Professional sports\", \"Amateur sports\", \"Olympic sports\", \"Non-Olympic sports\"]\n",
    "articles_dict[\"Technology\"] = [\"Technology\", \"Information technology\", \"Computer science\", \"Biotechnology\", \"Nanotechnology\", \"Robotics\", \"Artificial intelligence\", \"Machine learning\", \"Data science\", \"Software engineering\", \"Hardware engineering\", \"Electronics\", \"Telecommunications\", \"Internet\", \"World Wide Web\", \"Cybersecurity\", \"Cloud computing\", \"Quantum computing\", \"Virtual reality\", \"Augmented reality\", \"Wearable technology\", \"Mobile technology\", \"Smartphones\", \"Tablets\", \"Computers\", \"Laptops\", \"Networking\", \"Social media\", \"Digital marketing\", \"Search engine optimization\", \"Big data\", \"Blockchain\", \"Cryptocurrency\", \"Bitcoin\", \"Ethereum\", \"Smart contracts\", \"Internet of Things (IoT)\", \"Autonomous vehicles\", \"Drones\", \"3D printing\", \"CAD/CAM\", \"Consumer electronics\", \"Gaming technology\", \"Video games\", \"E-sports\", \"Software development\", \"Programming languages\", \"Operating systems\", \"Database management\", \"Data privacy\", \"Technology standards\", \"Technology patents\", \"Technology innovation\", \"Technology transfer\", \"Technology policy\", \"Technology regulation\", \"Technology ethics\", \"Technology history\", \"Technology trends\", \"Future of technology\", \"Tech companies\", \"Silicon Valley\", \"Startup companies\", \"Venture capital\", \"Product design\", \"User experience (UX)\", \"User interface (UI)\", \"Digital transformation\", \"Tech entrepreneurship\", \"Tech education\", \"STEM education\", \"Technology in education\", \"Technology in healthcare\", \"Technology in finance\", \"Green technology\", \"Sustainable technology\", \"Renewable energy technologies\"]\n",
    "articles_dict[\"Entertainment\"] = [\"Entertainment\", \"Film\", \"Television\", \"Music\", \"Video games\", \"Books\", \"Theatre\", \"Dance\", \"Circus\", \"Opera\", \"Concert\", \"Radio\", \"Podcasts\", \"Streaming media\", \"Blogging\", \"Vlogging\", \"Social media\", \"Comedy\", \"Stand-up comedy\", \"Magic (illusion)\", \"Illusion\", \"Performance art\", \"Visual arts\", \"Photography\", \"Cinematography\", \"Animation\", \"Anime\", \"Manga\", \"Comic books\", \"Graphic novels\", \"Webcomics\", \"Festivals\", \"Parades\", \"Nightclubs\", \"Parties\", \"Theme parks\", \"Amusement parks\", \"Casinos\", \"Gambling\", \"Board games\", \"Card games\", \"Puzzle games\", \"Reality television\", \"Talent shows\", \"Game shows\", \"Talk shows\", \"Documentaries\", \"Biopics\", \"Sports entertainment\", \"Professional wrestling\", \"Live music\", \"Concert tours\", \"Music festivals\", \"Film festivals\", \"Award shows\", \"Carnivals\", \"Fashion shows\", \"Art exhibitions\", \"Museum exhibitions\", \"Literary readings\", \"Book signings\", \"Theatrical productions\", \"Broadway shows\", \"Off-Broadway\", \"Musical theatre\", \"Drama\", \"Tragedy\", \"Comedy theatre\", \"Historical reenactment\", \"Cosplay\", \"Fan conventions\", \"Fan fiction\", \"Celebrity culture\", \"Celebrity news\", \"Pop culture\", \"Cultural events\", \"Public speaking\", \"Storytelling\", \"Entertainment industry\", \"Entertainment law\", \"Entertainment marketing\", \"Media production\", \"Event planning\", \"Entertainment technology\", \"Stagecraft\", \"Sound design\", \"Lighting design\", \"Special effects\", \"Costume design\", \"Set design\", \"Scriptwriting\", \"Directing\", \"Producing\", \"Choreography\", \"Entertainment criticism\", \"Entertainment journalism\", \"Entertainment awards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "def get_clean_content(page_title):\n",
    "    try:\n",
    "        # Fetch page content\n",
    "        content = wikipedia.page(page_title).content\n",
    "\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page not found: {page_title}\")\n",
    "        return ''\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # print(e.options)\n",
    "        # print(f\"Disambiguation error for {page_title}, chose {e.options[1]} instead\")\n",
    "        # content = wikipedia.page(e.options[1]).content\n",
    "        print(f\"Page not found: {page_title}\")\n",
    "        return ''\n",
    "\n",
    "    # Exclude content after specific headings like \"References\", \"See also\", etc.\n",
    "    excluded_headings = ['== References ==', '== See also ==', '== Sources ==', '== External links ==']\n",
    "    for heading in excluded_headings:\n",
    "        content = content.split(heading)[0]\n",
    "\n",
    "    # Remove all headings from the content\n",
    "    content = ''.join(line for line in content.splitlines(True) if not line.startswith('='))\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/154 [00:14<05:21,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Cultural tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/154 [00:18<04:51,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Dark tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/154 [00:19<04:30,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Gastronomic tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 12/154 [00:27<05:27,  2.30s/it]C:\\Users\\axelv\\AppData\\Roaming\\Python\\Python311\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\axelv\\AppData\\Roaming\\Python\\Python311\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "  8%|▊         | 13/154 [00:30<05:37,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Hotel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 19/154 [00:45<05:23,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Passport\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 21/154 [00:50<05:16,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Safari\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 23/154 [00:54<04:56,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Space tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 35/154 [01:23<04:08,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel photography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 37/154 [01:26<03:49,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Vacation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 39/154 [01:31<04:05,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Voluntourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 40/154 [01:33<03:47,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: World Heritage site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 43/154 [01:39<03:34,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Adventure tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 44/154 [01:40<03:20,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Agritourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 47/154 [01:47<03:40,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Educational tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 51/154 [01:55<03:06,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Religious tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 52/154 [01:56<02:56,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Slum tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 53/154 [01:57<02:24,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Space tourism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 69/154 [02:31<02:53,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior questionnaire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 71/154 [02:35<02:42,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 88/154 [03:14<02:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 95/154 [03:29<02:04,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 106/154 [03:54<01:40,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior professionals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 107/154 [03:56<01:31,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior consultants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 113/154 [04:08<01:18,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 123/154 [04:31<01:03,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior links\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 131/154 [04:49<00:47,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior directions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 139/154 [05:07<00:31,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 141/154 [05:11<00:25,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior key points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 142/154 [05:12<00:22,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior key takeaways\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 144/154 [05:15<00:16,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 151/154 [05:31<00:06,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not found: Travel behavior attachments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [05:38<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "\n",
    "# Variable to hold all the text combined\n",
    "full_text = {}\n",
    "for label in LABELS:\n",
    "    full_text[label] = \"\"\n",
    "\n",
    "# Loop through all the labels, and for each in all the articles, get the content and append it to the full text and save it to a file\n",
    "for i in range(3,4):\n",
    "    label = LABELS[i]\n",
    "    for page_title in tqdm.tqdm(articles_dict[label]):\n",
    "        full_text[label] += get_clean_content(page_title)\n",
    "\n",
    "    # Save the full text to a file\n",
    "    with open(f'{label}.txt', 'w') as f:\n",
    "        f.write(full_text[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full_text from the files if necessary\n",
    "full_text = {}\n",
    "for label in LABELS:\n",
    "    with open(f'{label}.txt', 'r', encoding='utf-8') as f:\n",
    "        full_text[label] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text\n",
      "0     The role of credit scores in lending decisions...\n",
      "1     The impact of overpopulation on the environmen...\n",
      "2     The importance of the scientific method in con...\n",
      "3     The startup accelerator provides funding and m...\n",
      "4     The benefits of biomimicry are many, including...\n",
      "...                                                 ...\n",
      "1135  The rock band's farewell tour marks the end of...\n",
      "1136  The impact of the gig economy on travel experi...\n",
      "1137  The importance of food safety cannot be overst...\n",
      "1138  The American Cancer Society recommends avoidin...\n",
      "1139  The quantum computer processes complex calcula...\n",
      "\n",
      "[1140 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import the data data/test_shuffle.txt and run the BM25 search on each line, and save the first label of recommendations to a csv file with headers ID (start at 0) and Label\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../../data/test_shuffle.txt', sep='\\t', header=None)\n",
    "data.columns = ['Text']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting best quality sentences from the extract of Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\axelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 151047\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import random\n",
    "\n",
    "# Download the pretrained Punkt tokenizer for English\n",
    "nltk.download('punkt')\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Create a dataset of sentences with their corresponding labels\n",
    "train_sentences = []\n",
    "for label in LABELS:\n",
    "    text = full_text[label].replace('\\n', ' ')\n",
    "    list_of_sentences = tokenizer.tokenize(text)\n",
    "    # Shuffle the list of sentences to have a more diverse dataset\n",
    "    random.shuffle(list_of_sentences)\n",
    "    # Select sentences ensuring they have a reasonable length\n",
    "    for sentence in list_of_sentences:\n",
    "        if 5 < len(sentence.split()) < 30:\n",
    "            train_sentences.append((sentence, label))\n",
    "\n",
    "print(f\"Number of sentences: {len(train_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\axelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\axelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\axelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\axelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 151047/151047 [00:31<00:00, 4828.37it/s]\n",
      "100%|██████████| 1140/1140 [00:00<00:00, 6531.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log2\n",
    "import random\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "# Download necessary NLTK models\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sentence_entropy(sentence):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    freqs = Counter(words)\n",
    "    total_count = sum(freqs.values())\n",
    "    probabilities = np.array(list(freqs.values())) / total_count\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def get_sentence_features(sentences):\n",
    "    features = []\n",
    "    for sentence, _ in tqdm.tqdm(sentences):  # Unpack the sentence and ignore the label for feature extraction\n",
    "        entropy = sentence_entropy(sentence)\n",
    "        num_words = len(sentence.split())\n",
    "        features.append((num_words, entropy))\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Assuming train_sentences is a list of tuples (sentence, label)\n",
    "train_features = get_sentence_features(train_sentences)\n",
    "test_features = get_sentence_features([(text, '') for text in data['Text']])  # dummy label for test features\n",
    "\n",
    "# Normalize the features\n",
    "train_features = (train_features - np.mean(train_features, axis=0)) / np.std(train_features, axis=0)\n",
    "test_features = (test_features - np.mean(test_features, axis=0)) / np.std(test_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sentences: 151047, After filtering: 151047\n",
      "Processing label: Politics, Sentences handled: 22923\n",
      "Processing label: Health, Sentences handled: 15750\n",
      "Processing label: Finance, Sentences handled: 13863\n",
      "Processing label: Travel, Sentences handled: 12199\n",
      "Processing label: Food, Sentences handled: 6927\n",
      "Not enough sentences for label 'Food'. Required: 7500, Available: 6927\n",
      "Processing label: Education, Sentences handled: 10179\n",
      "Processing label: Environment, Sentences handled: 10615\n",
      "Processing label: Fashion, Sentences handled: 8517\n",
      "Processing label: Science, Sentences handled: 12524\n",
      "Processing label: Sports, Sentences handled: 13311\n",
      "Processing label: Technology, Sentences handled: 12800\n",
      "Processing label: Entertainment, Sentences handled: 11439\n",
      "Selected Train Sentences for Modeling:\n",
      "Politics: The goal is to integrate them into the state, but only as reflections of the state's preferred culture.\n",
      "Politics: In the decades following the United States' invasion of Cuba in 1898, and formal independence from the U.S.\n",
      "Politics: Complicating Lincoln's attempts to defuse the crisis were the actions of the new Secretary of State, William Seward.\n",
      "Politics: Even so, nearly every time the Cuban military fought against the revolutionaries, the army was forced to retreat.\n",
      "Politics: Due process rights claimed to be at risk are:  The right to privacy, including a right to contraceptives.\n",
      "Politics: The scientific study of those interactions is also referred to as international studies, international politics, or international affairs.\n",
      "Politics: Fiscal stance: The size of the deficit or surplus Tax policy: The taxes used to collect government income.\n",
      "Politics: The Promotion of Black Self-Government Act of 1959 entrenched the NP policy of nominally independent \"homelands\" for blacks.\n",
      "Politics: The conservatives formed the Austrian People's Party, which has been the major conservative party in Austria ever since.\n",
      "Politics: The aid supplies, if allowed, will allow the Syrian population to protect themselves from contracting the COVID-19 virus.\n"
     ]
    }
   ],
   "source": [
    "def prefilter_features_by_distribution(train_features, test_features, z_score_threshold=10):\n",
    "    \"\"\"\n",
    "    Prefilter the train features by removing those that are too far from the test features' distribution.\n",
    "    \"\"\"\n",
    "    test_mean = np.mean(test_features, axis=0)\n",
    "    test_std = np.std(test_features, axis=0)\n",
    "    # Calculate Z-scores of train features\n",
    "    z_scores = np.abs((train_features - test_mean) / test_std)\n",
    "    filtered_indices = np.all(z_scores < z_score_threshold, axis=1)\n",
    "    return filtered_indices\n",
    "\n",
    "def select_similar_sentences(train_features, test_features, train_data, labels, num_samples=3000):\n",
    "    \"\"\"\n",
    "    Select num_samples sentences from the train set for each label,\n",
    "    that are closest to the test set distribution in terms of number of words, entropy, and named entities.\n",
    "    \"\"\"\n",
    "    selected_sentences = []\n",
    "    filtered_indices = prefilter_features_by_distribution(train_features, test_features)\n",
    "    filtered_train_features = train_features[filtered_indices]\n",
    "    filtered_train_data = [train_data[i] for i in range(len(train_data)) if filtered_indices[i]]\n",
    "\n",
    "    print(f\"Initial sentences: {len(train_features)}, After filtering: {len(filtered_train_features)}\")\n",
    "\n",
    "    for label in labels:\n",
    "        label_indices = [i for i, data in enumerate(filtered_train_data) if data[1] == label]\n",
    "        label_features = filtered_train_features[label_indices]\n",
    "        label_data = [filtered_train_data[i] for i in label_indices]\n",
    "\n",
    "        print(f\"Processing label: {label}, Sentences handled: {len(label_features)}\")\n",
    "        num_samples_label = num_samples\n",
    "        if len(label_features) < num_samples:\n",
    "            print(f\"Not enough sentences for label '{label}'. Required: {num_samples}, Available: {len(label_features)}\")\n",
    "            num_samples_label = len(label_features)\n",
    "            # continue\n",
    "\n",
    "        distances = cdist(label_features, test_features, metric='euclidean')\n",
    "        distances = np.mean(distances, axis=1)\n",
    "        indices = np.argsort(distances, axis=0)[:num_samples_label]\n",
    "        selected_sentences.extend([label_data[idx] for idx in indices.flatten()])\n",
    "\n",
    "    return selected_sentences\n",
    "\n",
    "\n",
    "# Select sentences similar to the test dataset, maintaining labels\n",
    "selected_train_sentences = select_similar_sentences(train_features, test_features, train_sentences, LABELS, num_samples=7500)\n",
    "\n",
    "# Output selected sentences along with their labels\n",
    "print(\"Selected Train Sentences for Modeling:\")\n",
    "for sentence, label in selected_train_sentences[:10]:\n",
    "    print(f\"{label}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating sentences embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all the sentences into embeddings 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' from SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Get the embeddings\n",
    "train_embeddings = []\n",
    "for sentence, label in tqdm.tqdm(selected_train_sentences):\n",
    "\ttrain_embeddings.append(model.encode(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings and the labels to a numpy file\n",
    "# np.save('train_embeddings.npy', train_embeddings)\n",
    "# np.save('train_labels.npy', [label for sentence, label in selected_train_sentences])\n",
    "\n",
    "# Load the embeddings and the labels\n",
    "train_embeddings = np.load('train_embeddings.npy')\n",
    "train_labels = np.load('train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test sentences in embeddings\n",
    "test_embeddings = []\n",
    "for i in range(len(data)):\n",
    "\ttest_embeddings.append(model.encode(data['Text'][i]))\n",
    "\n",
    "np.save('test_embeddings.npy', test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM model on the embeddings to predict the labels of the test dataset\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the SVM model with a rbf kernel \n",
    "clf = SVC(kernel='poly', C=10, gamma='scale')\n",
    "clf.fit(train_embeddings, [label for sentence, label in selected_train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_halving_search_cv  # Required to enable HalvingGridSearchCV\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Assuming train_embeddings and train_labels are defined\n",
    "# train_labels = [label for sentence, label in train_sentences]  # Adjust according to your data structure\n",
    "\n",
    "# # Set up the parameter grid to tune the SVM classifier\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10],  # Regularization parameter\n",
    "#     'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "#     'kernel': ['rbf', 'linear', 'poly', 'sigmoid']  # Type of kernel\n",
    "# }\n",
    "\n",
    "# Create a HalvingGridSearchCV object\n",
    "# grid_search = HalvingGridSearchCV(SVC(), param_grid, cv=5, verbose=2, scoring='accuracy', n_jobs=-1, factor=2, min_resources=\"exhaust\")\n",
    "# grid_search.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Best model after grid search\n",
    "# clf = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters and best training score\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test dataset and save them to a csv file\n",
    "predictions = clf.predict(test_embeddings)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['Label'])\n",
    "predictions_df.to_csv('result_extract/result_extract/SVM_v3.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a KNN to estimate the labels of the test_embeddings\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric = 'cosine')\n",
    "knn.fit(train_embeddings, [label for sentence, label in selected_train_sentences])\n",
    "\n",
    "# Predict the labels of the test dataset and save them to a csv file\n",
    "predictions = knn.predict(test_embeddings)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['Label'])\n",
    "predictions_df.to_csv('KNN_v1.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any NaN values or infinities\n",
    "if np.any(pd.isnull(train_labels)):\n",
    "    print(\"NaN values found in train_labels, need to clean or fill\")\n",
    "    train_labels = np.nan_to_num(train_labels)  # Replace nan with 0 and inf with finite numbers\n",
    "\n",
    "# Ensure all entries are of the same data type (e.g., all integers or all strings)\n",
    "print(\"Unique labels:\", np.unique(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming train_embeddings and test_embeddings are defined\n",
    "# Ensure train_labels are integer encoded\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(train_embeddings, train_labels_encoded)\n",
    "\n",
    "# Predict the labels of the test dataset\n",
    "predictions = xgb.predict(test_embeddings)\n",
    "predictions_labels = label_encoder.inverse_transform(predictions)  # Convert back to original labels if necessary\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df = pd.DataFrame(predictions_labels, columns=['Label'])\n",
    "predictions_df.to_csv('XGBoost_v1.csv', index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_BM25(corpus):\n",
    "    # We need to use lists for the BM25Okapi function, so we convert the dictionnaries to lists and keep both in the data_BM25 tuple\n",
    "    paragraphs_list = list(corpus.values())\n",
    "    IDs_list = list(corpus.keys())\n",
    "\n",
    "    # We define the tokenizer for BM25\n",
    "    def BM25_tokenizer(text):\n",
    "        tokenized_doc = []\n",
    "        for token in text.lower().split():\n",
    "            token = token.strip(string.punctuation)\n",
    "            tokenized_doc.append(token)\n",
    "        return tokenized_doc\n",
    "\n",
    "    # We use the tokenizer on the paragraphs\n",
    "    chunks_token = []\n",
    "    for paragraph in paragraphs_list:\n",
    "        chunks_token.append(BM25_tokenizer(paragraph))\n",
    "\n",
    "    # We create the BM25 model\n",
    "    bm25 = BM25Okapi(chunks_token)\n",
    "    data_BM25 = (bm25,IDs_list)\n",
    "    return data_BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input preprocessing for BM25, which is the tokenizer\n",
    "def input_preprocessing_BM25(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "        tokenized_doc.append(token)\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of BM25 search (lexical search), Work in progress, maybe will also implement other lexical search methods \n",
    "def reco_BM25(input_prompt, data_BM25, input_preprocessing_BM25, n_recommendations_BM25=1):\n",
    "    # We extract the bm25 model and the IDs list from the data_BM25 tuple, and compute the scores for each chunk\n",
    "    bm25,IDs_list = data_BM25\n",
    "    BM25_scores = bm25.get_scores(input_preprocessing_BM25(input_prompt))\n",
    "    \n",
    "    # We extract the n_recommendations best scores' indexes and return the corresponding IDs, sorted from the best to the worst\n",
    "    best_scores_indexes = np.argsort(BM25_scores)[-n_recommendations_BM25:][::-1]\n",
    "    recommendations_IDs = [IDs_list[i] for i in best_scores_indexes]\n",
    "    return recommendations_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data for BM25\n",
    "data_BM25 = data_preprocessing_BM25(full_text)\n",
    "\n",
    "# Get the recommendations for each line\n",
    "recommendations = []\n",
    "for i in range(len(data)):\n",
    "\trecommendations.append(reco_BM25(data['Text'][i], data_BM25, input_preprocessing_BM25, 1)[0])\n",
    "\n",
    "# Save the recommendations to a csv file\n",
    "recommendations_df = pd.DataFrame(recommendations, columns=['Label'])\n",
    "recommendations_df.to_csv('BM25_v1.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def data_preprocessing_TFIDF(chunks_dico):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    paragraphs_list = list(chunks_dico.values())\n",
    "    IDs_list = list(chunks_dico.keys())\n",
    "    tfidf_matrix = vectorizer.fit_transform(paragraphs_list)\n",
    "    return (vectorizer, tfidf_matrix, IDs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def reco_TFIDF(input_prompt, data_TFIDF, n_recommendations_TFIDF=1):\n",
    "\t# We extract the tfidf model and the IDs list from the data_TFIDF tuple, and compute the scores for each chunk\n",
    "    vectorizer, tfidf, IDs_list = data_TFIDF\n",
    "    query_tfidf = vectorizer.transform([input_prompt])\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf).flatten()\n",
    "    \n",
    "    # We extract the n_recommendations best scores' indexes and return the corresponding IDs, sorted from the best to the worst\n",
    "    best_scores_indexes = np.argsort(cosine_similarities)[-n_recommendations_TFIDF:][::-1]\n",
    "    recommendations_IDs = [IDs_list[i] for i in best_scores_indexes]\n",
    "\n",
    "    return recommendations_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the sentences in a dictionary with the labels, one big text for each label\n",
    "corpus = {}\n",
    "for sentence, label in selected_train_sentences:\n",
    "\tcorpus[label] = corpus.get(label,\"\") + sentence+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data for TFIDF\n",
    "data_TFIDF = data_preprocessing_TFIDF(corpus)\n",
    "\n",
    "# Get the recommendations for each line\n",
    "recommendations = []\n",
    "for i in range(len(data)):\n",
    "\trecommendations.append(reco_TFIDF(data['Text'][i], data_TFIDF)[0])\n",
    "\n",
    "# Save the recommendations to a csv file\n",
    "recommendations_df = pd.DataFrame(recommendations, columns=['Label'])\n",
    "recommendations_df.to_csv('TFIDF_v1.csv', index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
